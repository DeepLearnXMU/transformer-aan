{"eval_steps": 5000, "learning_rate_values": [0.0], "keep_checkpoint_max": 5, "clip_grad_norm": 0.0, "initializer": "uniform_unit_scaling", "filter_size": 2048, "attention_key_channels": 0, "references": ["../../dev.32k.de"], "decode_normalize": false, "warmup_steps": 4000, "train_steps": 100000, "attention_value_channels": 0, "constant_batch_size": false, "eval_batch_size": 32, "attention_dropout": 0.0, "relu_dropout": 0.0, "layer_preprocess": "none", "update_cycle": 8, "num_heads": 8, "max_length": 256, "pad": "<pad>", "save_checkpoint_secs": 0, "learning_rate_boundaries": [0], "save_checkpoint_steps": 1500, "learning_rate_decay": "noam", "num_encoder_layers": 6, "learning_rate": 1.0, "decode_alpha": 0.6, "eval_secs": 0, "top_beams": 1, "label_smoothing": 0.1, "adam_beta1": 0.9, "adam_beta2": 0.98, "decode_length": 50, "append_eos": false, "batch_size": 3125, "input": ["../../train.32k.en.shuf", "../../train.32k.de.shuf"], "buffer_size": 10000, "shared_embedding_and_softmax_weights": true, "multiply_embedding_mode": "sqrt_depth", "adam_epsilon": 1e-09, "length_multiplier": 1, "vocab": ["../../vocab.32k.en.txt", "../../vocab.32k.de.txt"], "num_threads": 6, "hidden_size": 512, "num_decoder_layers": 6, "device_list": [3], "mantissa_bits": 2, "bos": "<eos>", "residual_dropout": 0.1, "decode_constant": 5.0, "validation": "../../dev.32k.en", "eos": "<eos>", "record": "", "beam_size": 4, "keep_top_checkpoint_max": 1, "initializer_gain": 1.0, "shared_source_target_embedding": false, "output": "train", "model": "transformer", "unk": "<unk>", "layer_postprocess": "layer_norm"}